{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i used https://www.kaggle.com/abhishek/distilbert-use-features-oof kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/sacremoses/sacremoses-master/ > /dev/null\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import math\n",
    "sys.path.insert(0, \"../input/transformers/transformers-master/\")\n",
    "import transformers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import MultiTaskElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qa_id', 'question_title', 'question_body', 'question_user_name',\n",
       "       'question_user_page', 'answer', 'answer_user_name', 'answer_user_page',\n",
       "       'url', 'category', 'host'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../input/google-quest-challenge\")\n",
    "root=\"../input/google-quest-challenge/\"\n",
    "train=pd.read_csv(root+\"train.csv\").fillna(\"none\")\n",
    "test=pd.read_csv(root+\"test.csv\").fillna(\"none\")\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets=['question_asker_intent_understanding',\n",
    "       'question_body_critical', 'question_conversational',\n",
    "       'question_expect_short_answer', 'question_fact_seeking',\n",
    "       'question_has_commonly_accepted_answer',\n",
    "       'question_interestingness_others', 'question_interestingness_self',\n",
    "       'question_multi_intent', 'question_not_really_a_question',\n",
    "       'question_opinion_seeking', 'question_type_choice',\n",
    "       'question_type_compare', 'question_type_consequence',\n",
    "       'question_type_definition', 'question_type_entity',\n",
    "       'question_type_instructions', 'question_type_procedure',\n",
    "       'question_type_reason_explanation', 'question_type_spelling',\n",
    "       'question_well_written', 'answer_helpful',\n",
    "       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "       'answer_satisfaction', 'answer_type_instructions',\n",
    "       'answer_type_procedure', 'answer_type_reason_explanation',\n",
    "       'answer_well_written']\n",
    "features=[c for c in test.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">feature engneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6079, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get netloc\n",
    "find=re.compile(r\"^[^.]*\")\n",
    "train[\"netloc\"] = train[\"url\"].apply(lambda x: re.findall(find,urlparse(x).netloc)[0])\n",
    "test['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "#one hot encoder\n",
    "cat_cols=[\"netloc\",\"category\"]\n",
    "merged=pd.concat([train[cat_cols],test[cat_cols]])\n",
    "ohe=OneHotEncoder().fit(merged)\n",
    "ohe_train=ohe.transform(train[cat_cols]).toarray()\n",
    "ohe_test=ohe.transform(test[cat_cols]).toarray()\n",
    "ohe_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_title\n",
      "question_body\n",
      "answer\n",
      "CPU times: user 57min 27s, sys: 3min 55s, total: 1h 1min 23s\n",
      "Wall time: 18min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "302230"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#embedding\n",
    "module_url=\"../input/universalsentenceencoderlarge4/\"\n",
    "embed = hub.load(module_url)\n",
    "em_cols=['question_title', 'question_body', 'answer']\n",
    "\n",
    "embeddings_train = {}\n",
    "embeddings_test = {}\n",
    "for text in em_cols:\n",
    "    print(text)\n",
    "    train_text = train[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    test_text = test[text].str.replace('?', '.').str.replace('!', '.').tolist()\n",
    "    \n",
    "    curr_train_emb = []\n",
    "    curr_test_emb = []\n",
    "    batch_size = 4\n",
    "    ind = 0\n",
    "    while ind*batch_size < len(train_text):\n",
    "        curr_train_emb.append(embed(train_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1\n",
    "        \n",
    "    ind = 0\n",
    "    while ind*batch_size < len(test_text):\n",
    "        curr_test_emb.append(embed(test_text[ind*batch_size: (ind + 1)*batch_size])[\"outputs\"].numpy())\n",
    "        ind += 1    \n",
    "        \n",
    "    embeddings_train[text + '_embedding'] = np.vstack(curr_train_emb)\n",
    "    embeddings_test[text + '_embedding'] = np.vstack(curr_test_emb)\n",
    "    \n",
    "del embed\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_dist = lambda x, y: np.power(x - y, 2).sum(axis=1)\n",
    "\n",
    "cos_dist = lambda x, y: (x*y).sum(axis=1)\n",
    "\n",
    "dist_features_train = np.array([\n",
    "    l2_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    l2_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding']),\n",
    "    cos_dist(embeddings_train['question_title_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['answer_embedding']),\n",
    "    cos_dist(embeddings_train['question_body_embedding'], embeddings_train['question_title_embedding'])\n",
    "]).T\n",
    "\n",
    "dist_features_test = np.array([\n",
    "    l2_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    l2_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding']),\n",
    "    cos_dist(embeddings_test['question_title_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['answer_embedding']),\n",
    "    cos_dist(embeddings_test['question_body_embedding'], embeddings_test['question_title_embedding'])\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6079, 1606)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.hstack([i for k,i in embeddings_train.items()]+[ohe_train,dist_features_train])\n",
    "X_test=np.hstack([i for k,i in embeddings_test.items()]+[ohe_test,dist_features_test])\n",
    "y_train=train[targets].values\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatible with tensorflow backend\n",
    "class SpearmanRhoCallback(Callback):\n",
    "    def __init__(self, training_data, validation_data, patience, model_name):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.value = -1\n",
    "        self.bad_epochs = 0\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
    "        if rho_val >= self.value:\n",
    "            self.value = rho_val\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "        if self.bad_epochs >= self.patience:\n",
    "            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "            #self.model.save_weights(self.model_name)\n",
    "        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n",
    "        return rho_val\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    inputs = Input(shape=(X_train.shape[1],))\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x=Dense(256,activation=\"relu\")(x)\n",
    "    x = Dense(y_train.shape[1], activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=1e-4),\n",
    "        loss=['binary_crossentropy']\n",
    "    )\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1606)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               822784    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 961,822\n",
      "Trainable params: 961,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 2s 343us/step - loss: 0.5130 - val_loss: 0.4200\n",
      "val_spearman-rho: 0.173                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 275us/step - loss: 0.4047 - val_loss: 0.4006\n",
      "val_spearman-rho: 0.2644                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 277us/step - loss: 0.3906 - val_loss: 0.3918\n",
      "val_spearman-rho: 0.3036                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 1s 274us/step - loss: 0.3820 - val_loss: 0.3861\n",
      "val_spearman-rho: 0.3268                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 1s 277us/step - loss: 0.3763 - val_loss: 0.3827\n",
      "val_spearman-rho: 0.3394                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 1s 274us/step - loss: 0.3722 - val_loss: 0.3805\n",
      "val_spearman-rho: 0.3487                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 267us/step - loss: 0.3691 - val_loss: 0.3788\n",
      "val_spearman-rho: 0.3555                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 1s 282us/step - loss: 0.3664 - val_loss: 0.3779\n",
      "val_spearman-rho: 0.3605                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 289us/step - loss: 0.3639 - val_loss: 0.3768\n",
      "val_spearman-rho: 0.3643                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 286us/step - loss: 0.3619 - val_loss: 0.3763\n",
      "val_spearman-rho: 0.3672                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 286us/step - loss: 0.3602 - val_loss: 0.3755\n",
      "val_spearman-rho: 0.3694                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 1s 282us/step - loss: 0.3582 - val_loss: 0.3751\n",
      "val_spearman-rho: 0.3716                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 290us/step - loss: 0.3566 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3733                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 293us/step - loss: 0.3549 - val_loss: 0.3745\n",
      "val_spearman-rho: 0.3746                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 1s 297us/step - loss: 0.3532 - val_loss: 0.3744\n",
      "val_spearman-rho: 0.3759                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3516 - val_loss: 0.3743\n",
      "val_spearman-rho: 0.3767                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 285us/step - loss: 0.3503 - val_loss: 0.3742\n",
      "val_spearman-rho: 0.3773                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 1s 281us/step - loss: 0.3487 - val_loss: 0.3742\n",
      "val_spearman-rho: 0.3781                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 281us/step - loss: 0.3472 - val_loss: 0.3743\n",
      "val_spearman-rho: 0.3786                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3456 - val_loss: 0.3745\n",
      "val_spearman-rho: 0.3785                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 1s 283us/step - loss: 0.3445 - val_loss: 0.3746\n",
      "val_spearman-rho: 0.379                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 1s 287us/step - loss: 0.3431 - val_loss: 0.3748\n",
      "val_spearman-rho: 0.3792                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 1s 291us/step - loss: 0.3416 - val_loss: 0.3749\n",
      "val_spearman-rho: 0.3792                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 290us/step - loss: 0.3405 - val_loss: 0.3756\n",
      "val_spearman-rho: 0.3789                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 1s 285us/step - loss: 0.3390 - val_loss: 0.3754\n",
      "val_spearman-rho: 0.3794                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 1s 289us/step - loss: 0.3377 - val_loss: 0.3763\n",
      "val_spearman-rho: 0.3789                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 1s 288us/step - loss: 0.3365 - val_loss: 0.3761\n",
      "Epoch 00026: early stopping Threshold\n",
      "val_spearman-rho: 0.3788                                                                                                    \n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 1606)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               822784    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 961,822\n",
      "Trainable params: 961,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 2s 340us/step - loss: 0.5101 - val_loss: 0.4180\n",
      "val_spearman-rho: 0.1845                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 289us/step - loss: 0.4058 - val_loss: 0.3980\n",
      "val_spearman-rho: 0.273                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3907 - val_loss: 0.3879\n",
      "val_spearman-rho: 0.3139                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 1s 296us/step - loss: 0.3823 - val_loss: 0.3821\n",
      "val_spearman-rho: 0.3337                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 2s 310us/step - loss: 0.3767 - val_loss: 0.3786\n",
      "val_spearman-rho: 0.3457                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 1s 307us/step - loss: 0.3726 - val_loss: 0.3766\n",
      "val_spearman-rho: 0.353                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 308us/step - loss: 0.3696 - val_loss: 0.3753\n",
      "val_spearman-rho: 0.358                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 1s 304us/step - loss: 0.3665 - val_loss: 0.3739\n",
      "val_spearman-rho: 0.3613                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 306us/step - loss: 0.3643 - val_loss: 0.3735\n",
      "val_spearman-rho: 0.364                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 305us/step - loss: 0.3619 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.3655                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 281us/step - loss: 0.3599 - val_loss: 0.3724\n",
      "val_spearman-rho: 0.3673                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 1s 274us/step - loss: 0.3582 - val_loss: 0.3720\n",
      "val_spearman-rho: 0.3684                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 278us/step - loss: 0.3562 - val_loss: 0.3718\n",
      "val_spearman-rho: 0.3696                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 280us/step - loss: 0.3550 - val_loss: 0.3718\n",
      "val_spearman-rho: 0.3705                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 1s 279us/step - loss: 0.3533 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.371                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 272us/step - loss: 0.3517 - val_loss: 0.3719\n",
      "val_spearman-rho: 0.3716                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 282us/step - loss: 0.3500 - val_loss: 0.3721\n",
      "val_spearman-rho: 0.3723                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 1s 283us/step - loss: 0.3489 - val_loss: 0.3719\n",
      "val_spearman-rho: 0.3724                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 281us/step - loss: 0.3474 - val_loss: 0.3718\n",
      "val_spearman-rho: 0.3726                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 277us/step - loss: 0.3459 - val_loss: 0.3719\n",
      "val_spearman-rho: 0.3729                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 1s 276us/step - loss: 0.3446 - val_loss: 0.3722\n",
      "val_spearman-rho: 0.3728                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3433 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3728                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 1s 278us/step - loss: 0.3423 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.3731                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 281us/step - loss: 0.3408 - val_loss: 0.3728\n",
      "val_spearman-rho: 0.373                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 1s 281us/step - loss: 0.3395 - val_loss: 0.3735\n",
      "val_spearman-rho: 0.3728                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 1s 279us/step - loss: 0.3382 - val_loss: 0.3734\n",
      "Epoch 00025: early stopping Threshold\n",
      "val_spearman-rho: 0.3723                                                                                                    \n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 1606)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               822784    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 961,822\n",
      "Trainable params: 961,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 2s 325us/step - loss: 0.5255 - val_loss: 0.4181\n",
      "val_spearman-rho: 0.1565                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 278us/step - loss: 0.4086 - val_loss: 0.3993\n",
      "val_spearman-rho: 0.2478                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 289us/step - loss: 0.3943 - val_loss: 0.3895\n",
      "val_spearman-rho: 0.2974                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 1s 294us/step - loss: 0.3853 - val_loss: 0.3828\n",
      "val_spearman-rho: 0.3232                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 1s 292us/step - loss: 0.3788 - val_loss: 0.3789\n",
      "val_spearman-rho: 0.3386                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 1s 290us/step - loss: 0.3743 - val_loss: 0.3762\n",
      "val_spearman-rho: 0.3486                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 292us/step - loss: 0.3708 - val_loss: 0.3750\n",
      "val_spearman-rho: 0.3546                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 1s 287us/step - loss: 0.3683 - val_loss: 0.3734\n",
      "val_spearman-rho: 0.3599                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 291us/step - loss: 0.3654 - val_loss: 0.3728\n",
      "val_spearman-rho: 0.3642                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 285us/step - loss: 0.3634 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.3681                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 289us/step - loss: 0.3613 - val_loss: 0.3716\n",
      "val_spearman-rho: 0.3701                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3596 - val_loss: 0.3713\n",
      "val_spearman-rho: 0.3721                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 278us/step - loss: 0.3580 - val_loss: 0.3709\n",
      "val_spearman-rho: 0.3733                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3562 - val_loss: 0.3709\n",
      "val_spearman-rho: 0.375                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 1s 282us/step - loss: 0.3548 - val_loss: 0.3704\n",
      "val_spearman-rho: 0.3759                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 283us/step - loss: 0.3532 - val_loss: 0.3702\n",
      "val_spearman-rho: 0.3764                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 288us/step - loss: 0.3518 - val_loss: 0.3705\n",
      "val_spearman-rho: 0.377                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 1s 287us/step - loss: 0.3502 - val_loss: 0.3703\n",
      "val_spearman-rho: 0.3776                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 290us/step - loss: 0.3490 - val_loss: 0.3704\n",
      "val_spearman-rho: 0.3777                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 292us/step - loss: 0.3475 - val_loss: 0.3704\n",
      "val_spearman-rho: 0.3782                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 1s 290us/step - loss: 0.3464 - val_loss: 0.3706\n",
      "val_spearman-rho: 0.3785                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3449 - val_loss: 0.3710\n",
      "val_spearman-rho: 0.378                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 1s 283us/step - loss: 0.3439 - val_loss: 0.3709\n",
      "val_spearman-rho: 0.3778                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3423 - val_loss: 0.3713\n",
      "val_spearman-rho: 0.3784                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 1s 277us/step - loss: 0.3412 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.3784                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 1s 277us/step - loss: 0.3400 - val_loss: 0.3721\n",
      "Epoch 00025: early stopping Threshold\n",
      "val_spearman-rho: 0.3781                                                                                                    \n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1606)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               822784    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 961,822\n",
      "Trainable params: 961,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/100\n",
      "4863/4863 [==============================] - 2s 315us/step - loss: 0.5189 - val_loss: 0.4163\n",
      "val_spearman-rho: 0.1773                                                                                                    \n",
      "Epoch 2/100\n",
      "4863/4863 [==============================] - 1s 278us/step - loss: 0.4055 - val_loss: 0.3950\n",
      "val_spearman-rho: 0.2738                                                                                                    \n",
      "Epoch 3/100\n",
      "4863/4863 [==============================] - 1s 279us/step - loss: 0.3909 - val_loss: 0.3860\n",
      "val_spearman-rho: 0.3116                                                                                                    \n",
      "Epoch 4/100\n",
      "4863/4863 [==============================] - 1s 283us/step - loss: 0.3827 - val_loss: 0.3809\n",
      "val_spearman-rho: 0.3319                                                                                                    \n",
      "Epoch 5/100\n",
      "4863/4863 [==============================] - 1s 290us/step - loss: 0.3771 - val_loss: 0.3780\n",
      "val_spearman-rho: 0.3433                                                                                                    \n",
      "Epoch 6/100\n",
      "4863/4863 [==============================] - 1s 288us/step - loss: 0.3736 - val_loss: 0.3752\n",
      "val_spearman-rho: 0.3516                                                                                                    \n",
      "Epoch 7/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3705 - val_loss: 0.3739\n",
      "val_spearman-rho: 0.3577                                                                                                    \n",
      "Epoch 8/100\n",
      "4863/4863 [==============================] - 1s 287us/step - loss: 0.3678 - val_loss: 0.3729\n",
      "val_spearman-rho: 0.3626                                                                                                    \n",
      "Epoch 9/100\n",
      "4863/4863 [==============================] - 1s 283us/step - loss: 0.3654 - val_loss: 0.3719\n",
      "val_spearman-rho: 0.3666                                                                                                    \n",
      "Epoch 10/100\n",
      "4863/4863 [==============================] - 1s 282us/step - loss: 0.3633 - val_loss: 0.3708\n",
      "val_spearman-rho: 0.3693                                                                                                    \n",
      "Epoch 11/100\n",
      "4863/4863 [==============================] - 1s 282us/step - loss: 0.3615 - val_loss: 0.3711\n",
      "val_spearman-rho: 0.3714                                                                                                    \n",
      "Epoch 12/100\n",
      "4863/4863 [==============================] - 1s 278us/step - loss: 0.3596 - val_loss: 0.3699\n",
      "val_spearman-rho: 0.374                                                                                                    \n",
      "Epoch 13/100\n",
      "4863/4863 [==============================] - 1s 282us/step - loss: 0.3579 - val_loss: 0.3695\n",
      "val_spearman-rho: 0.3756                                                                                                    \n",
      "Epoch 14/100\n",
      "4863/4863 [==============================] - 1s 280us/step - loss: 0.3563 - val_loss: 0.3693\n",
      "val_spearman-rho: 0.3767                                                                                                    \n",
      "Epoch 15/100\n",
      "4863/4863 [==============================] - 1s 275us/step - loss: 0.3547 - val_loss: 0.3695\n",
      "val_spearman-rho: 0.3776                                                                                                    \n",
      "Epoch 16/100\n",
      "4863/4863 [==============================] - 1s 273us/step - loss: 0.3533 - val_loss: 0.3692\n",
      "val_spearman-rho: 0.3788                                                                                                    \n",
      "Epoch 17/100\n",
      "4863/4863 [==============================] - 1s 280us/step - loss: 0.3516 - val_loss: 0.3693\n",
      "val_spearman-rho: 0.3797                                                                                                    \n",
      "Epoch 18/100\n",
      "4863/4863 [==============================] - 1s 279us/step - loss: 0.3503 - val_loss: 0.3693\n",
      "val_spearman-rho: 0.3801                                                                                                    \n",
      "Epoch 19/100\n",
      "4863/4863 [==============================] - 1s 281us/step - loss: 0.3490 - val_loss: 0.3692\n",
      "val_spearman-rho: 0.3804                                                                                                    \n",
      "Epoch 20/100\n",
      "4863/4863 [==============================] - 1s 284us/step - loss: 0.3473 - val_loss: 0.3695\n",
      "val_spearman-rho: 0.3807                                                                                                    \n",
      "Epoch 21/100\n",
      "4863/4863 [==============================] - 1s 283us/step - loss: 0.3462 - val_loss: 0.3697\n",
      "val_spearman-rho: 0.3811                                                                                                    \n",
      "Epoch 22/100\n",
      "4863/4863 [==============================] - 1s 287us/step - loss: 0.3449 - val_loss: 0.3698\n",
      "val_spearman-rho: 0.3818                                                                                                    \n",
      "Epoch 23/100\n",
      "4863/4863 [==============================] - 1s 281us/step - loss: 0.3435 - val_loss: 0.3699\n",
      "val_spearman-rho: 0.382                                                                                                    \n",
      "Epoch 24/100\n",
      "4863/4863 [==============================] - 1s 280us/step - loss: 0.3423 - val_loss: 0.3703\n",
      "val_spearman-rho: 0.382                                                                                                    \n",
      "Epoch 25/100\n",
      "4863/4863 [==============================] - 1s 282us/step - loss: 0.3409 - val_loss: 0.3710\n",
      "val_spearman-rho: 0.3821                                                                                                    \n",
      "Epoch 26/100\n",
      "4863/4863 [==============================] - 1s 279us/step - loss: 0.3398 - val_loss: 0.3710\n",
      "val_spearman-rho: 0.3818                                                                                                    \n",
      "Epoch 27/100\n",
      "4863/4863 [==============================] - 1s 278us/step - loss: 0.3386 - val_loss: 0.3714\n",
      "val_spearman-rho: 0.3819                                                                                                    \n",
      "Epoch 28/100\n",
      "4863/4863 [==============================] - 1s 276us/step - loss: 0.3372 - val_loss: 0.3712\n",
      "val_spearman-rho: 0.3821                                                                                                    \n",
      "Epoch 29/100\n",
      "4863/4863 [==============================] - 1s 272us/step - loss: 0.3360 - val_loss: 0.3719\n",
      "val_spearman-rho: 0.3817                                                                                                    \n",
      "Epoch 30/100\n",
      "4863/4863 [==============================] - 1s 273us/step - loss: 0.3350 - val_loss: 0.3721\n",
      "Epoch 00029: early stopping Threshold\n",
      "val_spearman-rho: 0.3814                                                                                                    \n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 1606)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 512)               822784    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 30)                7710      \n",
      "=================================================================\n",
      "Total params: 961,822\n",
      "Trainable params: 961,822\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4864 samples, validate on 1215 samples\n",
      "Epoch 1/100\n",
      "4864/4864 [==============================] - 2s 316us/step - loss: 0.5128 - val_loss: 0.4139\n",
      "val_spearman-rho: 0.1707                                                                                                    \n",
      "Epoch 2/100\n",
      "4864/4864 [==============================] - 1s 281us/step - loss: 0.4063 - val_loss: 0.3946\n",
      "val_spearman-rho: 0.2602                                                                                                    \n",
      "Epoch 3/100\n",
      "4864/4864 [==============================] - 1s 281us/step - loss: 0.3922 - val_loss: 0.3866\n",
      "val_spearman-rho: 0.2983                                                                                                    \n",
      "Epoch 4/100\n",
      "4864/4864 [==============================] - 1s 287us/step - loss: 0.3844 - val_loss: 0.3816\n",
      "val_spearman-rho: 0.3196                                                                                                    \n",
      "Epoch 5/100\n",
      "4864/4864 [==============================] - 1s 285us/step - loss: 0.3786 - val_loss: 0.3782\n",
      "val_spearman-rho: 0.3313                                                                                                    \n",
      "Epoch 6/100\n",
      "4864/4864 [==============================] - 1s 282us/step - loss: 0.3743 - val_loss: 0.3761\n",
      "val_spearman-rho: 0.3407                                                                                                    \n",
      "Epoch 7/100\n",
      "4864/4864 [==============================] - 1s 287us/step - loss: 0.3709 - val_loss: 0.3744\n",
      "val_spearman-rho: 0.3462                                                                                                    \n",
      "Epoch 8/100\n",
      "4864/4864 [==============================] - 1s 294us/step - loss: 0.3680 - val_loss: 0.3735\n",
      "val_spearman-rho: 0.351                                                                                                    \n",
      "Epoch 9/100\n",
      "4864/4864 [==============================] - 1s 289us/step - loss: 0.3655 - val_loss: 0.3726\n",
      "val_spearman-rho: 0.3548                                                                                                    \n",
      "Epoch 10/100\n",
      "4864/4864 [==============================] - 1s 287us/step - loss: 0.3634 - val_loss: 0.3723\n",
      "val_spearman-rho: 0.357                                                                                                    \n",
      "Epoch 11/100\n",
      "4864/4864 [==============================] - 1s 287us/step - loss: 0.3614 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.3589                                                                                                    \n",
      "Epoch 12/100\n",
      "4864/4864 [==============================] - 1s 286us/step - loss: 0.3593 - val_loss: 0.3713\n",
      "val_spearman-rho: 0.3603                                                                                                    \n",
      "Epoch 13/100\n",
      "4864/4864 [==============================] - 1s 288us/step - loss: 0.3578 - val_loss: 0.3712\n",
      "val_spearman-rho: 0.3619                                                                                                    \n",
      "Epoch 14/100\n",
      "4864/4864 [==============================] - 1s 280us/step - loss: 0.3560 - val_loss: 0.3710\n",
      "val_spearman-rho: 0.3629                                                                                                    \n",
      "Epoch 15/100\n",
      "4864/4864 [==============================] - 1s 286us/step - loss: 0.3544 - val_loss: 0.3713\n",
      "val_spearman-rho: 0.3628                                                                                                    \n",
      "Epoch 16/100\n",
      "4864/4864 [==============================] - 1s 280us/step - loss: 0.3529 - val_loss: 0.3710\n",
      "val_spearman-rho: 0.3636                                                                                                    \n",
      "Epoch 17/100\n",
      "4864/4864 [==============================] - 1s 285us/step - loss: 0.3509 - val_loss: 0.3714\n",
      "val_spearman-rho: 0.3637                                                                                                    \n",
      "Epoch 18/100\n",
      "4864/4864 [==============================] - 1s 286us/step - loss: 0.3496 - val_loss: 0.3718\n",
      "val_spearman-rho: 0.3632                                                                                                    \n",
      "Epoch 19/100\n",
      "4864/4864 [==============================] - 1s 287us/step - loss: 0.3485 - val_loss: 0.3717\n",
      "val_spearman-rho: 0.3636                                                                                                    \n",
      "Epoch 20/100\n",
      "4864/4864 [==============================] - 1s 284us/step - loss: 0.3470 - val_loss: 0.3718\n",
      "val_spearman-rho: 0.3637                                                                                                    \n",
      "Epoch 21/100\n",
      "4864/4864 [==============================] - 1s 285us/step - loss: 0.3458 - val_loss: 0.3721\n",
      "val_spearman-rho: 0.3634                                                                                                    \n",
      "Epoch 22/100\n",
      "4864/4864 [==============================] - 1s 284us/step - loss: 0.3447 - val_loss: 0.3722\n",
      "Epoch 00021: early stopping Threshold\n",
      "val_spearman-rho: 0.3635                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "all_pres=[]\n",
    "kf=KFold(n_splits=5,shuffle=True)\n",
    "for fold_,(tr_id,val_id) in enumerate(kf.split(X_train)):\n",
    "    X_tr,y_tr = X_train[tr_id],y_train[tr_id]\n",
    "    X_val,y_val = X_train[val_id],y_train[val_id]\n",
    "    \n",
    "    model=create_model()\n",
    "    model.fit(X_tr,y_tr,epochs=100,batch_size=32,validation_data=(X_val,y_val),\n",
    "              verbose=True,callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr),validation_data=(X_val, y_val),\n",
    "                                                          patience=5, model_name=f'best_model_batch{ind}.h5')])\n",
    "    all_pres.append(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = np.array([np.array([rankdata(c) for c in p.T]).T for p in all_pres]).mean(axis=0)\n",
    "max_val = test_preds.max() + 0.1\n",
    "test_preds = test_preds/max_val + 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.605335</td>\n",
       "      <td>0.800252</td>\n",
       "      <td>0.963663</td>\n",
       "      <td>0.824617</td>\n",
       "      <td>0.067633</td>\n",
       "      <td>0.264650</td>\n",
       "      <td>0.695232</td>\n",
       "      <td>0.655325</td>\n",
       "      <td>0.414199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793531</td>\n",
       "      <td>0.508297</td>\n",
       "      <td>0.396975</td>\n",
       "      <td>0.496534</td>\n",
       "      <td>0.477631</td>\n",
       "      <td>0.472590</td>\n",
       "      <td>0.060491</td>\n",
       "      <td>0.101659</td>\n",
       "      <td>0.966604</td>\n",
       "      <td>0.706994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.464188</td>\n",
       "      <td>0.461668</td>\n",
       "      <td>0.045369</td>\n",
       "      <td>0.141987</td>\n",
       "      <td>0.800672</td>\n",
       "      <td>0.714976</td>\n",
       "      <td>0.591052</td>\n",
       "      <td>0.530141</td>\n",
       "      <td>0.360849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521319</td>\n",
       "      <td>0.546944</td>\n",
       "      <td>0.354967</td>\n",
       "      <td>0.483092</td>\n",
       "      <td>0.463768</td>\n",
       "      <td>0.455787</td>\n",
       "      <td>0.938458</td>\n",
       "      <td>0.679269</td>\n",
       "      <td>0.077295</td>\n",
       "      <td>0.321781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.616677</td>\n",
       "      <td>0.676749</td>\n",
       "      <td>0.464188</td>\n",
       "      <td>0.784709</td>\n",
       "      <td>0.822096</td>\n",
       "      <td>0.937198</td>\n",
       "      <td>0.603235</td>\n",
       "      <td>0.430162</td>\n",
       "      <td>0.812014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729259</td>\n",
       "      <td>0.502836</td>\n",
       "      <td>0.295316</td>\n",
       "      <td>0.653224</td>\n",
       "      <td>0.429742</td>\n",
       "      <td>0.366730</td>\n",
       "      <td>0.174333</td>\n",
       "      <td>0.269691</td>\n",
       "      <td>0.868725</td>\n",
       "      <td>0.464188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.301197</td>\n",
       "      <td>0.079395</td>\n",
       "      <td>0.216341</td>\n",
       "      <td>0.420080</td>\n",
       "      <td>0.867465</td>\n",
       "      <td>0.847301</td>\n",
       "      <td>0.280613</td>\n",
       "      <td>0.489393</td>\n",
       "      <td>0.890149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276833</td>\n",
       "      <td>0.724218</td>\n",
       "      <td>0.817895</td>\n",
       "      <td>0.662046</td>\n",
       "      <td>0.754883</td>\n",
       "      <td>0.791850</td>\n",
       "      <td>0.604495</td>\n",
       "      <td>0.855283</td>\n",
       "      <td>0.756984</td>\n",
       "      <td>0.258349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.944339</td>\n",
       "      <td>0.398656</td>\n",
       "      <td>0.590212</td>\n",
       "      <td>0.954421</td>\n",
       "      <td>0.509557</td>\n",
       "      <td>0.681790</td>\n",
       "      <td>0.878387</td>\n",
       "      <td>0.947280</td>\n",
       "      <td>0.846461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622558</td>\n",
       "      <td>0.846041</td>\n",
       "      <td>0.686410</td>\n",
       "      <td>0.906952</td>\n",
       "      <td>0.792271</td>\n",
       "      <td>0.689351</td>\n",
       "      <td>0.315060</td>\n",
       "      <td>0.738500</td>\n",
       "      <td>0.528040</td>\n",
       "      <td>0.859063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>245</td>\n",
       "      <td>0.569208</td>\n",
       "      <td>0.890149</td>\n",
       "      <td>0.841840</td>\n",
       "      <td>0.162571</td>\n",
       "      <td>0.814535</td>\n",
       "      <td>0.482672</td>\n",
       "      <td>0.710355</td>\n",
       "      <td>0.455787</td>\n",
       "      <td>0.769586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794791</td>\n",
       "      <td>0.340685</td>\n",
       "      <td>0.211720</td>\n",
       "      <td>0.128965</td>\n",
       "      <td>0.145348</td>\n",
       "      <td>0.366730</td>\n",
       "      <td>0.141147</td>\n",
       "      <td>0.519639</td>\n",
       "      <td>0.876707</td>\n",
       "      <td>0.112161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>257</td>\n",
       "      <td>0.393195</td>\n",
       "      <td>0.342365</td>\n",
       "      <td>0.388574</td>\n",
       "      <td>0.612896</td>\n",
       "      <td>0.527620</td>\n",
       "      <td>0.582651</td>\n",
       "      <td>0.189876</td>\n",
       "      <td>0.174753</td>\n",
       "      <td>0.632220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447385</td>\n",
       "      <td>0.868725</td>\n",
       "      <td>0.838479</td>\n",
       "      <td>0.683470</td>\n",
       "      <td>0.639782</td>\n",
       "      <td>0.786809</td>\n",
       "      <td>0.693972</td>\n",
       "      <td>0.860323</td>\n",
       "      <td>0.472170</td>\n",
       "      <td>0.510817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>267</td>\n",
       "      <td>0.839740</td>\n",
       "      <td>0.704474</td>\n",
       "      <td>0.986767</td>\n",
       "      <td>0.178954</td>\n",
       "      <td>0.389414</td>\n",
       "      <td>0.100399</td>\n",
       "      <td>0.972485</td>\n",
       "      <td>0.974585</td>\n",
       "      <td>0.653644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.494014</td>\n",
       "      <td>0.710775</td>\n",
       "      <td>0.602815</td>\n",
       "      <td>0.551565</td>\n",
       "      <td>0.493594</td>\n",
       "      <td>0.013022</td>\n",
       "      <td>0.037807</td>\n",
       "      <td>0.993489</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>284</td>\n",
       "      <td>0.275152</td>\n",
       "      <td>0.137366</td>\n",
       "      <td>0.399496</td>\n",
       "      <td>0.225163</td>\n",
       "      <td>0.527200</td>\n",
       "      <td>0.656165</td>\n",
       "      <td>0.342785</td>\n",
       "      <td>0.496954</td>\n",
       "      <td>0.800252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354127</td>\n",
       "      <td>0.659945</td>\n",
       "      <td>0.417559</td>\n",
       "      <td>0.522159</td>\n",
       "      <td>0.663726</td>\n",
       "      <td>0.697332</td>\n",
       "      <td>0.547784</td>\n",
       "      <td>0.763705</td>\n",
       "      <td>0.570048</td>\n",
       "      <td>0.466709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>292</td>\n",
       "      <td>0.728838</td>\n",
       "      <td>0.566688</td>\n",
       "      <td>0.352867</td>\n",
       "      <td>0.814115</td>\n",
       "      <td>0.754883</td>\n",
       "      <td>0.743121</td>\n",
       "      <td>0.807813</td>\n",
       "      <td>0.738080</td>\n",
       "      <td>0.476371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302457</td>\n",
       "      <td>0.234825</td>\n",
       "      <td>0.286074</td>\n",
       "      <td>0.547784</td>\n",
       "      <td>0.351187</td>\n",
       "      <td>0.239866</td>\n",
       "      <td>0.341105</td>\n",
       "      <td>0.204999</td>\n",
       "      <td>0.853602</td>\n",
       "      <td>0.141147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.605335                0.800252   \n",
       "1     46                             0.464188                0.461668   \n",
       "2     70                             0.616677                0.676749   \n",
       "3    132                             0.301197                0.079395   \n",
       "4    200                             0.944339                0.398656   \n",
       "5    245                             0.569208                0.890149   \n",
       "6    257                             0.393195                0.342365   \n",
       "7    267                             0.839740                0.704474   \n",
       "8    284                             0.275152                0.137366   \n",
       "9    292                             0.728838                0.566688   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.963663                      0.824617   \n",
       "1                 0.045369                      0.141987   \n",
       "2                 0.464188                      0.784709   \n",
       "3                 0.216341                      0.420080   \n",
       "4                 0.590212                      0.954421   \n",
       "5                 0.841840                      0.162571   \n",
       "6                 0.388574                      0.612896   \n",
       "7                 0.986767                      0.178954   \n",
       "8                 0.399496                      0.225163   \n",
       "9                 0.352867                      0.814115   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.067633                               0.264650   \n",
       "1               0.800672                               0.714976   \n",
       "2               0.822096                               0.937198   \n",
       "3               0.867465                               0.847301   \n",
       "4               0.509557                               0.681790   \n",
       "5               0.814535                               0.482672   \n",
       "6               0.527620                               0.582651   \n",
       "7               0.389414                               0.100399   \n",
       "8               0.527200                               0.656165   \n",
       "9               0.754883                               0.743121   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.695232                       0.655325   \n",
       "1                         0.591052                       0.530141   \n",
       "2                         0.603235                       0.430162   \n",
       "3                         0.280613                       0.489393   \n",
       "4                         0.878387                       0.947280   \n",
       "5                         0.710355                       0.455787   \n",
       "6                         0.189876                       0.174753   \n",
       "7                         0.972485                       0.974585   \n",
       "8                         0.342785                       0.496954   \n",
       "9                         0.807813                       0.738080   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.414199  ...               0.793531        0.508297   \n",
       "1               0.360849  ...               0.521319        0.546944   \n",
       "2               0.812014  ...               0.729259        0.502836   \n",
       "3               0.890149  ...               0.276833        0.724218   \n",
       "4               0.846461  ...               0.622558        0.846041   \n",
       "5               0.769586  ...               0.794791        0.340685   \n",
       "6               0.632220  ...               0.447385        0.868725   \n",
       "7               0.653644  ...               0.888889        0.494014   \n",
       "8               0.800252  ...               0.354127        0.659945   \n",
       "9               0.476371  ...               0.302457        0.234825   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.396975          0.496534          0.477631   \n",
       "1                     0.354967          0.483092          0.463768   \n",
       "2                     0.295316          0.653224          0.429742   \n",
       "3                     0.817895          0.662046          0.754883   \n",
       "4                     0.686410          0.906952          0.792271   \n",
       "5                     0.211720          0.128965          0.145348   \n",
       "6                     0.838479          0.683470          0.639782   \n",
       "7                     0.710775          0.602815          0.551565   \n",
       "8                     0.417559          0.522159          0.663726   \n",
       "9                     0.286074          0.547784          0.351187   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.472590                  0.060491               0.101659   \n",
       "1             0.455787                  0.938458               0.679269   \n",
       "2             0.366730                  0.174333               0.269691   \n",
       "3             0.791850                  0.604495               0.855283   \n",
       "4             0.689351                  0.315060               0.738500   \n",
       "5             0.366730                  0.141147               0.519639   \n",
       "6             0.786809                  0.693972               0.860323   \n",
       "7             0.493594                  0.013022               0.037807   \n",
       "8             0.697332                  0.547784               0.763705   \n",
       "9             0.239866                  0.341105               0.204999   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.966604             0.706994  \n",
       "1                        0.077295             0.321781  \n",
       "2                        0.868725             0.464188  \n",
       "3                        0.756984             0.258349  \n",
       "4                        0.528040             0.859063  \n",
       "5                        0.876707             0.112161  \n",
       "6                        0.472170             0.510817  \n",
       "7                        0.993489             0.869565  \n",
       "8                        0.570048             0.466709  \n",
       "9                        0.853602             0.141147  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(root+'sample_submission.csv')\n",
    "submission[targets] = test_preds\n",
    "submission.to_csv(\"submission.csv\", index = False)\n",
    "submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
